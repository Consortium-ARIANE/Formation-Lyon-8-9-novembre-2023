{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpIkBLOLP8VH"
      },
      "source": [
        "# `Formation Consortium Ariane: Lyon 8-9 novembre 2023`\n",
        "##### [Motasem Alrahabi](https://obtic.sorbonne-universite.fr/alrahabi/index.html), [ObTIC](https://obtic.sorbonne-universite.fr/) - [Sorbonne Université](https://www.sorbonne-universite.fr/)\n",
        "### `Lien Github:` https://github.com/Consortium-ARIANE/Formation-Lyon-8-9-novembre-2023/tree/main/carto\n",
        "\n",
        "### `Lien Colab:` https://colab.research.google.com/drive/1Es7sDrnOnhLtjo-7OBraEqRkW32OZq7x?usp=sharing  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXaxzG5fJkCA"
      },
      "source": [
        "## `Introduction`\n",
        "\n",
        "*   Le traitement automatique des langues ([TAL](https://fr.wikipedia.org/wiki/Traitement_automatique_du_langage_naturel)) est un domaine qui permet aux ordinateurs de comprendre, de générer et de manipuler le langage humain.\n",
        "\n",
        "*   L'objectif du TAL est d'analyser les données langagières (écrites ou parlées), afin d'effectuer un certain nombre de tâches comme la classification, la génération, la réponse à des questions, le résumé, etc.\n",
        "\n",
        "*   Généralement, on applique une série de prétraitements aux données, afin de réaliser les différentes tâches: tokenisation (diviser le texte en \"tokens\"), nettoyage du texte (supprimer les caractères spéciaux, les symboles...), suppression des mots courants ou stopwords, conversion en minuscules, normalisation (lemmatisation ou désinence), encodage ou vectorisation (convertir les tokens en une représentation numérique adaptée à l'entrée du modèle), détection de la langue, etc.\n",
        "\n",
        "*   Les modèles pré-entrainés: modèles de neuronnes pré-entrainés sur un grand nombre de données.\n",
        "\n",
        "*   Les Transformers sont une classe d'architectures de réseaux de neurones artificiels. Ils ont la capacité de gérer des séquences de données, comme du texte, de manière plus efficace que les architectures précédentes.\n",
        "\n",
        "*   [Hugging Face](https://huggingface.co/) est une librairie qui fournit des API et des outils pour utiliser les modèles pré-entrainés.\n",
        "\n",
        "*   Hugging Face Pipeline simplifie l'utilisation de modèles pour des tâches courantes: classification, résumé, traduction, QR, etc.\n",
        "\n",
        "*   Dans cette formation on utilisera uniquement des modèles existants (une suite logique serait de former de nouveaux modèles: entrainement, évaluation, etc.).\n",
        "\n",
        "*   Pour la formation, on va utiliser google collab qui offre l'avantage d'utiliser des GPU gratuitement.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# `Partie 1: utiliser les transformers pour réaliser différentes tâches en TAL`:"
      ],
      "metadata": {
        "id": "ok1veHLCxma3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `1-1) Importer quelques librairies nécessaires`:"
      ],
      "metadata": {
        "id": "2Z5ZVjxmOG2A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYyje5tpKVWx"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q transformers\n",
        "!pip install -q sentencepiece\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification, AutoModelForCausalLM, AutoModelWithLMHead\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `1-2) Etiquetage morpho-syntaxique (parties du discours, POS)`:"
      ],
      "metadata": {
        "id": "0mPIrnGL9uS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gilf/french-camembert-postag-model\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"gilf/french-camembert-postag-model\")\n",
        "categories = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "texte = \"Je suis étudiant et je vis à Lyon\"\n",
        "categories(texte)\n",
        "#Liste des catégories: https://huggingface.co/gilf/french-camembert-postag-model"
      ],
      "metadata": {
        "id": "_7SlfkcX-ENY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_csDPxiHOZdM"
      },
      "source": [
        "### `1-3) Reconnaissance d'entités nommées (NER)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_u4hAKuMlAV"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
        "ner = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", grouped_entities=True)\n",
        "texte = \"Carla travaille à l'ONU à Paris\"\n",
        "pd.DataFrame(ner(texte))\n",
        "# https://huggingface.co/Davlan/bert-base-multilingual-cased-ner-hrl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwsVMgThFGhG"
      },
      "source": [
        "### `1-4) Classification de textes`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjzkALCyFMVm"
      },
      "outputs": [],
      "source": [
        "classifieur = pipeline(\"zero-shot-classification\")\n",
        "texte = \"Cette formation porte sur l'utilisation de la bibliothèque Hugging Face\"\n",
        "thematique = [\"technologie\", \"éducation\", \"société\", \"formation\"]\n",
        "classifieur(texte, thematique)\n",
        "#https://huggingface.co/tasks/zero-shot-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnDlqsAWOHgl"
      },
      "source": [
        "### `1-5) Analyse des sentiments et des émotions`:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utilisation du modèle par défaut\n",
        "classifieur = pipeline(\"sentiment-analysis\")\n",
        "classifieur(\"Cette formation est très cool. Je la recommande à mes amis\")"
      ],
      "metadata": {
        "id": "sYxR2ZZymQlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "py2ZV-UIKu4U"
      },
      "outputs": [],
      "source": [
        "# On peut aussi personnaliser le modèle:\n",
        "classifieur = pipeline(model=\"citizenlab/twitter-xlm-roberta-base-sentiment-finetunned\")\n",
        "classifieur(\"Cette formation est inutile. Je ne la recommande pas à mes amis\")\n",
        "# https://huggingface.co/citizenlab/twitter-xlm-roberta-base-sentiment-finetunned"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des émotions sur l'anglais (Ekman classif.):\n",
        "classifier = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\n",
        "classifier(\"I love using transformers. The best part is wide range of support and its easy to use\", )\n",
        "# https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion"
      ],
      "metadata": {
        "id": "4aFD3Z-36wKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# un autre exemple proche, et qui donne un seul résultat:\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
        "text = \"i feel as if i haven't blogged in ages or at least truly blogged. I am doing an update. Cute.\"# Output: 'joy'\n",
        "#text = \"i have a feeling i kinda lost my best friend\" # Output: 'sadness'\n",
        "tokenizer.decode(model.generate(tokenizer.encode(text + '</s', return_tensors='pt'), max_length=2)[0])\n",
        "# https://huggingface.co/mrm8488/t5-base-finetuned-emotion"
      ],
      "metadata": {
        "id": "edjQiurzJnGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des émotions sur l'anglais (goEmotions classif.):\n",
        "classifier = pipeline(\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n",
        "classifier(\"I am not having a great day\")\n",
        "# https://huggingface.co/SamLowe/roberta-base-go_emotions"
      ],
      "metadata": {
        "id": "VzWNDrTfmiSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QYvLGBsOTDe"
      },
      "source": [
        "### `1-6) Génération de textes`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAmenn_PMI81"
      },
      "outputs": [],
      "source": [
        "generateur = pipeline(\"text-generation\", model = \"bigscience/bloom-560m\")\n",
        "prompt = \"Cette formation va vous permettre de\"\n",
        "generateur(prompt, max_length = 40)\n",
        "# https://huggingface.co/bigscience/bloom-560m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f4ANx_-POum"
      },
      "source": [
        "### `1-7) Question-Réponse (QA)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNTc8F8BNGEM"
      },
      "outputs": [],
      "source": [
        "question_reponse = pipeline(\"question-answering\", model=\"cmarkea/distilcamembert-base-qa\", tokenizer=\"cmarkea/distilcamembert-base-qa\")\n",
        "texte = \"\"\"David Fincher, né le 28 août 1962 à Denver (Colorado),\n",
        "    est un réalisateur et producteur américain. Il est principalement\n",
        "    connu pour avoir réalisé les films Seven, Fight Club, L'Étrange\n",
        "    Histoire de Benjamin Button, The Social Network et Gone Girl qui\n",
        "    lui ont valu diverses récompenses et nominations aux Oscars du\n",
        "    cinéma ou aux Golden Globes. \"\"\"\n",
        "question_reponse(context = texte, question = \"Quel est le métier de David Fincher ?\")\n",
        "# https://huggingface.co/cmarkea/distilcamembert-base-qa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dcNwdprPoM_"
      },
      "source": [
        "### `1-8) Résumé automatique`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnAETP4sPuge"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, SummarizationPipeline\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"plguillou/t5-base-fr-sum-cnndm\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"plguillou/t5-base-fr-sum-cnndm\")\n",
        "resumeur = SummarizationPipeline(model=model, tokenizer=tokenizer)\n",
        "\n",
        "texte = \"\"\"David Fincher, né le 28 août 1962 à Denver (Colorado),\n",
        "    est un réalisateur et producteur américain. Il est principalement\n",
        "    connu pour avoir réalisé les films Seven, Fight Club, L'Étrange\n",
        "    Histoire de Benjamin Button, The Social Network et Gone Girl qui\n",
        "    lui ont valu diverses récompenses et nominations aux Oscars du\n",
        "    cinéma ou aux Golden Globes. Réputé pour son perfectionnisme, il\n",
        "    peut tourner un très grand nombre de prises de ses plans et\n",
        "    séquences afin d'obtenir le rendu visuel qu'il désire. Il a\n",
        "    également développé et produit les séries télévisées House of\n",
        "    Cards et Mindhunter, diffusées sur Netflix.\"\"\"\n",
        "\n",
        "resumeur(texte, min_length=20, max_length=100, clean_up_tokenization_spaces=True)\n",
        "# https://huggingface.co/plguillou/t5-base-fr-sum-cnndm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsxQXNkWPvkn"
      },
      "source": [
        "### `1-9) Traduction`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPS0cbr7PyQH"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "traducteur = pipeline(\"translation_en_to_fr\", model=model, tokenizer=tokenizer)\n",
        "traducteur(\"My name is Bob and I work in Japan\")\n",
        "# https://huggingface.co/t5-small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJULbx0IQPh4"
      },
      "source": [
        "### `1-10) Conversation`:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Conversation\n",
        "#chatbot = pipeline(\"conversational\")\n",
        "chatbot = pipeline(model=\"microsoft/DialoGPT-medium\")\n",
        "conversation = Conversation(\"I would like to watch a movie today, any suggestion?\")\n",
        "conversation = chatbot(conversation)\n",
        "print()\n",
        "print(\"Q - I would like to watch a movie today, any suggestion?\")\n",
        "print(\"A - \", conversation.generated_responses[-1])\n",
        "\n",
        "conversation.add_user_input(\"Is it an action movie?\")\n",
        "conversation = chatbot(conversation)\n",
        "print(\"Q - Is it an action movie?\")\n",
        "print(\"A - \", conversation.generated_responses[-1])\n",
        "# https://huggingface.co/microsoft/DialoGPT-medium?text=Hi."
      ],
      "metadata": {
        "id": "3sa0w32erfXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `1-11) Comparaison`:"
      ],
      "metadata": {
        "id": "XqXB_Keobjy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model =  SentenceTransformer(\"dangvantuan/sentence-camembert-large\")\n",
        "sentences = [\"Un avion est en train de décoller.\",\n",
        "          \"Un homme joue d'une grande flûte.\",\n",
        "          \"Un homme étale du fromage râpé sur une pizza.\",\n",
        "          \"Une personne jette un chat au plafond.\",\n",
        "          \"Une personne est en train de plier un morceau de papier.\",\n",
        "          ]\n",
        "model.encode(sentences)\n",
        "# https://huggingface.co/dangvantuan/sentence-camembert-large"
      ],
      "metadata": {
        "id": "P0HA7YRcbpWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# `Partie 2: Cartographie émotionnelle des noms de lieux`"
      ],
      "metadata": {
        "id": "RDxNAuPZwaxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#`Introduction`:\n",
        "La cartographie émotionnelle des toponymes est une méthode qui consiste à associer des émotions à des lieux géographiques spécifiques.\n",
        "\n",
        "Cette méthode peut être utilisée pour explorer la manière dont les lieux sont perçus et ressentis par les gens qui y vivent ou qui les visitent.\n",
        "\n",
        "Exemples: [Victorian London](https://www.historypin.org/en/victorian-london/geo/51.5128,-0.116085,12/bounds/51.423936,-0.222,51.601491,-0.01017/paging/1) project, [Mapping Arabia](https://www.google.com/maps/d/u/0/edit?hl=fr&mid=1M--Wq2CJcCPjIccrm5h8LYoTANTTH4cc&ll=48.84129625249132%2C2.356532365466477&z=13), etc.\n",
        "\n",
        "\n",
        "Pour générer une carte avec les noms de lieu qui se trouvent dans un texte, on peut suivre les étpaes suivantes:\n",
        "\n",
        "1- **`Extraire les noms de lieu du texte`** en utilisant des techniques de NLP pour détecter les entités géographiques dans le texte: noms de villes, de pays, de rivières, de montagnes, etc.\n",
        "\n",
        "Difficultés: Couverture: les EN sont une liste ouverte...(mots inconnus, variantes graphiques et historiques, abréviations...) ; Granularité et délimitation: imbrication, portée...(jardin du Luxembourg) ; Homonymie: Paris (France) vs Paris Hilton ; Orange (ville) vs Orange (fruit) ; Métonymique: Charles de Gaulle (personne, aéroport, porte-avion, etc.)...\n",
        "\n",
        "2- **`Géocodage`** : envoyer les noms de lieu à un service de géocodage pour convertir les noms de lieu en coordonnées géographiques (latitude et longitude). On peut le faire en utilisant une bibliothèque de géocodage ou en faisant des requêtes à un service de géocodage en ligne.\n",
        "\n",
        "Difficultés: la désambiguïsation des noms de lieu qui se trouvent en doublons. Ex: Paris (France) vs Paris (Etas-Unis) ; Tunis (pays) ou Tunis (capitale). On peut aussi assurer la liaison de données (linking data) qui permet de relier à un référentiel unique une ou plusieurs entités (Ex. 1 av. de l'Exemple, Paris -> 1 avenue de l'Exemple, 75005 Paris). On utilise des bases comme Wikidata.\n",
        "\n",
        "3- **`Affichage sur une carte`** : afficher les coordonnées géographiques des lieux sur une carte en utilisant une bibliothèque de cartographie comme Leaflet, Google Maps, Mapbox, etc. On pourra créer des marqueurs sur la carte pour chaque lieu et y afficher le nom du lieu en tant qu'infobulle ou étiquette.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TMbhiY9OwByG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ce code effectue les opérations suivantes :**\n",
        "1. Le texte en entrée est segmenté, phrase par phrase (avec Spacy).\n",
        "2. Chaque phrase est parcourue pour toutes les entités nommées LOC (avec Spacy). Il est possible de remplacer le modèle utilisé par défaut avec un autre. Une liste de termes indésirables peut également être personnalisée par l'utilisateur.\n",
        "3. Pour chaque entité trouvée, le script recherche les coordonnées géographiques. Il utilise la base de données Geonamescache comme source principale. En cas d'absence de résultat, il se connecte au serveur Nominatim afin de géocoder les lieux (les villes dans ce script).\n",
        "4. Si au moins une entité nommée est trouvée dans une phrase, une nouvelle fonction analyse les sentiments positifs, négatifs et neutres de la phrase entière (en utilisant Bert-base-multilingual-uncased-sentiment - qui peut d'ailleurs être remplacé par un autre modèle).\n",
        "5. A l'aide de la bibliothèque Python Folium (basée sur LeafLet), le script affiche sur une carte les entités nommées trouvées. La couleur de chaque icône est calculée en fonction du sentiment moyen de cette entité dans l'ensemble du texte : vert pour positif, rouge pour négatif et gris pour neutre. La taille de l'icône est proportionnelle au nombre d'occurrences dans l'ensemble du texte. Pour chaque icône, une fenêtre contextuelle affiche le nom du lieu, le numéro d'occurrence, le nombre de sentiments positifs, négatifs et neutres dans le texte saisi.\n",
        "6. Le script enregistre chaque entité dans un fichier csv avec les informations suivantes : fichier, phrase, sentiment, latitude et longueur.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vvm2-gQ10UKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `0) Quelques exercices avant de commencer`:"
      ],
      "metadata": {
        "id": "T2GWO0DVzyk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour rechercher les coordonnées géographiques d'un nom de lieu:"
      ],
      "metadata": {
        "id": "pawUAH_xQC9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install geopy\n",
        "from geopy.geocoders import Nominatim\n",
        "geolocator = Nominatim(user_agent='my-app', timeout=10)\n",
        "location = geolocator.geocode(\"Lyon\")\n",
        "print(\"Coordonnées de Lyon : Latitude:\", location.latitude, \" / Longitude:\", location.longitude)"
      ],
      "metadata": {
        "id": "dyk8MNre1FM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour effectuer une recherche inverse (reverse geocoding) et récupérer le nom de lieu à partir de ses coordonnées géographiques:"
      ],
      "metadata": {
        "id": "qGy9hC-qPeNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latitude = 45.75\n",
        "longitude = 4.85\n",
        "location = geolocator.reverse((latitude, longitude), exactly_one=True)\n",
        "if location is not None:\n",
        "    print(f\"Nom de lieu le plus proche : {location.address}\")\n",
        "else:\n",
        "    print(\"Aucun lieu trouvé pour les coordonnées spécifiées.\")"
      ],
      "metadata": {
        "id": "V08JBvFb2DnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour afficher la ville de Lyon sur une carte :\n"
      ],
      "metadata": {
        "id": "EeQI6wbqQf6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install folium\n",
        "import folium\n",
        "lyon_map = folium.Map(location=[45.75, 4.85])\n",
        "folium.Marker(location=[45.75, 4.85], popup=\"Formation Consortium ARIANE - Lyon\").add_to(lyon_map)\n",
        "lyon_map"
      ],
      "metadata": {
        "id": "DuKlhZfGNnTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Analyser des sentiments autour des noms de lieu\n"
      ],
      "metadata": {
        "id": "696c81CmCTre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `1) Télécharger les bibliothèques nécessaires`:"
      ],
      "metadata": {
        "id": "8DAOgGO9wOrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q spacy geopy folium transformers\n",
        "! pip install -q --upgrade tensorflow\n",
        "! pip install -q geonamescache\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import spacy\n",
        "import folium\n",
        "import requests\n",
        "import geonamescache\n",
        "\n",
        "from folium import plugins\n",
        "from spacy import displacy\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "from google.colab import files, output"
      ],
      "metadata": {
        "id": "FbV5wmmOwILv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `2) Charger le modèle de reconnaissance d'entités nommées`:"
      ],
      "metadata": {
        "id": "Rj-pHYVvkceU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m spacy download fr_core_news_md\n",
        "nlp = spacy.load('fr_core_news_md') # l'utilisateur peut ici charger le modèle qu'il souhaite."
      ],
      "metadata": {
        "id": "BZpyPodCwIIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `3) Charger le modèle d'analyse de sentiments`:"
      ],
      "metadata": {
        "id": "qi0yJJQVkjYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "sentiment_analysis_model = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", framework=\"pt\")\n",
        "#https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"
      ],
      "metadata": {
        "id": "Ry1RQFhHwHHM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `4) Charger une liste de terms d'entités nommées à exclure`:\n"
      ],
      "metadata": {
        "id": "Q13gX7_dkm5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blacklist = [\"XVIIIe\", \"XVIe\"] # juste pour l'exemple"
      ],
      "metadata": {
        "id": "Qxs1PXwLko8c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `5) Segmenter le texte en phrases`:"
      ],
      "metadata": {
        "id": "3T4aaS1hkxdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segmenter_en_phrases(text):\n",
        "  all_sentences = []\n",
        "  text = text.replace('’', \"'\") # normalisation\n",
        "  doc = nlp(text)\n",
        "  sentences = [sent.text for sent in doc.sents]\n",
        "  all_sentences.extend(sentences)\n",
        "  #print(all_sentences) # toutes les phrases du corpus dans une liste\n",
        "  return all_sentences"
      ],
      "metadata": {
        "id": "MsUBmBHhk13Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `6) Traiter chaque phrase: identifier les entités, les coordonnées et les sentiments`."
      ],
      "metadata": {
        "id": "AqOpcgQNk4mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentence(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  analyzed_sentence = []\n",
        "  entities = []\n",
        "  latitude = None\n",
        "  longitude = None\n",
        "  sentiment_score_sentence = 0\n",
        "  sentiment_label_sentence = \"\"\n",
        "\n",
        "  for ent in doc.ents:\n",
        "      if ent.label_ == \"LOC\":  # je ne prends pas les autres types d'entités (PER, ORG...)\n",
        "        if ent.text.lower() not in blacklist:\n",
        "            entities.append((ent.text, ent.start, ent.end))\n",
        "\n",
        "            # Charger la bdd geonames:\n",
        "            gc = geonamescache.GeonamesCache()\n",
        "            matching_cities = gc.get_cities_by_name(ent.text)\n",
        "\n",
        "            if matching_cities: #seulement les villes dans ce script\n",
        "                first_matching_city = list(matching_cities[0].values())[0]\n",
        "                latitude = first_matching_city['latitude']\n",
        "                longitude = first_matching_city['longitude']\n",
        "            else:\n",
        "                # Si la ville n'st pas trouvée dans geonamescache, utiliser Nominatim comme une deuxième option:\n",
        "                geolocator = Nominatim(user_agent='my-app', timeout=10)\n",
        "                try:\n",
        "                    location = geolocator.geocode(ent.text)\n",
        "                    if location:\n",
        "                      latitude = location.latitude\n",
        "                      longitude = location.longitude\n",
        "                    else:\n",
        "                      raise GeocoderTimedOut(\"Geocoding timed out for location using Nominatim: \" + ent.text)\n",
        "                except GeocoderTimedOut as e:\n",
        "                  print(f\"Geocoding timed out for location using Nominatim: {ent.text}\")\n",
        "                  latitude = None\n",
        "                  longitude = None\n",
        "                  location = None\n",
        "\n",
        "            if entities and latitude and longitude:\n",
        "              sentiment_score_sentence = sentiment_analysis_model(sentence)[0][\"score\"]\n",
        "              sentiment_label_sentence = sentiment_analysis_model(sentence)[0]['label']\n",
        "            result = {\n",
        "                \"entity\": ent.text,\n",
        "                \"sentiment_label_sentence\": sentiment_label_sentence,\n",
        "                \"sentiment_score_sentence\": sentiment_score_sentence,\n",
        "                \"latitude\": latitude,\n",
        "                \"longitude\": longitude\n",
        "            }\n",
        "            analyzed_sentence.append(result)\n",
        "\n",
        "  return analyzed_sentence"
      ],
      "metadata": {
        "id": "MW0ZCHmLk5Wm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `7) Charger le corpus: choisir l'une de ces 2 options`:\n",
        "\n",
        "### 7-1) Le texte input est fourni dans le code\n"
      ],
      "metadata": {
        "id": "Sl-rZivQk7W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"J'adore Paris pour son atmosphère romantique et ses délicieuses pâtisseries.\n",
        "New York m'impressionne avec ses gratte-ciel imposants, mais je trouve la ville un peu trop bruyante à mon goût.\n",
        "La campagne française est d'une beauté paisible qui apaise l'âme.\n",
        "Je suis neutre à l'égard de Los Angeles, la ville a ses avantages, mais le trafic peut être vraiment agaçant.\n",
        "Venise m'a profondément ému avec ses canaux pittoresques et son histoire fascinante.\n",
        "Londres a une ambiance unique qui me rend nostalgique de mes voyages passés.\n",
        "Je n'aime pas du tout les hivers rigoureux de Montréal, mais l'été est absolument charmant.\n",
        "La plage de Bali est un véritable paradis, et j'adore passer du temps là-bas.\n",
        "Les déserts du Sahara sont à couper le souffle, mais la chaleur peut être accablante.\n",
        "Tokyo m'a laissé une impression mitigée, certains quartiers sont incroyablement modernes, tandis que d'autres conservent une atmosphère traditionnelle qui est fascinante.\"\"\"\n",
        "\n",
        "all_sentences = segmenter_en_phrases(text)"
      ],
      "metadata": {
        "id": "aJ5XC2ielMGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7-2) Le texte est un fichier sur PC"
      ],
      "metadata": {
        "id": "SaFDG4UalPNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "file_name = next(iter(uploaded))\n",
        "if os.path.isfile(file_name) and file_name.endswith(\".txt\"):\n",
        "  with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "    all_sentences = segmenter_en_phrases(text)\n",
        "else:\n",
        "  print(f\"Le fichier {file_name} n'est pas au format .txt et ne sera pas traité.\")"
      ],
      "metadata": {
        "id": "UiFn6fyzlRkr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "9bd9506e-100d-41a8-dd7d-0d75ab481802"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-563b1ef0-e8d7-4b12-b4b0-643f870b440e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-563b1ef0-e8d7-4b12-b4b0-643f870b440e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Stefan Zweig MAGELLAN.txt to Stefan Zweig MAGELLAN (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `8) Parcourir et agréger les résultats`:"
      ],
      "metadata": {
        "id": "HaQSz88FlWZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spécifier ici une limite de phrases à traiter (pour réduire le temps de traitement):\n",
        "limite = 100\n",
        "\n",
        "# Enregistrr les entités dans un CSV file:\n",
        "analyzed_sentences = []\n",
        "\n",
        "with open('output.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=\"\\t\")\n",
        "    writer.writerow(['sentence', 'entity', 'label', 'score', 'latitude', 'longitude'])\n",
        "    counter = 0\n",
        "    # Analyze each sentence\n",
        "    for sentence in all_sentences:\n",
        "      if counter>=limite: # le nombre maximum de phrases à analyser\n",
        "        break\n",
        "      analyzed_sentence = analyze_sentence(sentence)\n",
        "      for result in analyzed_sentence:\n",
        "        entity = result[\"entity\"]\n",
        "        sentiment_label = result[\"sentiment_label_sentence\"]\n",
        "        sentiment_score = result[\"sentiment_score_sentence\"]\n",
        "        latitude = result[\"latitude\"]\n",
        "        longitude = result[\"longitude\"]\n",
        "\n",
        "        # check if lat & long are None\n",
        "        if latitude is None or longitude is None:\n",
        "            print(f\"Skipping result due to missing location data for entity: {entity}\")\n",
        "            continue  # skip this sentence and move to the next one\n",
        "\n",
        "        writer.writerow([sentence, entity, sentiment_label, round(sentiment_score, 2), latitude, longitude])\n",
        "        counter += 1\n",
        "        print(f\"Sentiment de la phrase {counter}: {sentence[:20]}... ; entité:[{entity}] ; polarity: {sentiment_label} ; score: {round(sentiment_score, 2)}\")\n",
        "        analyzed_sentences.append({\n",
        "            'sentence': sentence,\n",
        "            'entity': entity,\n",
        "            'sentiment_label': sentiment_label,\n",
        "            'sentiment_score': round(sentiment_score, 2),\n",
        "            'latitude': latitude,\n",
        "            'longitude': longitude\n",
        "        })\n",
        "\n",
        "\n",
        "# Itérer sur les analyzed_sentences et créer un nouveau dico:\n",
        "entities_dict = {}\n",
        "for sentence_info in analyzed_sentences:\n",
        "    entity = sentence_info['entity']\n",
        "    sentiment_label = sentence_info['sentiment_label']\n",
        "\n",
        "    # Initialisere les infos de l'entités si ce n'est pas encore fait\n",
        "    if entity not in entities_dict:\n",
        "        entities_dict[entity] = {\n",
        "            'latitude': sentence_info['latitude'],\n",
        "            'longitude': sentence_info['longitude'],\n",
        "            'occurrences': 1,\n",
        "            'positive_labels': 0,\n",
        "            'negative_labels': 0,\n",
        "            'neutral_labels': 0,\n",
        "            'overall_sentiment': sentiment_label\n",
        "        }\n",
        "    else:\n",
        "        entities_dict[entity]['occurrences'] = entities_dict[entity]['occurrences'] + 1\n",
        "\n",
        "    # màj le calcul des sentiments\n",
        "    if sentiment_label in ['4 stars', '5 stars']:\n",
        "        entities_dict[entity]['positive_labels'] += 1\n",
        "    elif sentiment_label in ['1 star', '2 stars']:\n",
        "        entities_dict[entity]['negative_labels'] += 1\n",
        "    elif sentiment_label == '3 stars':\n",
        "        entities_dict[entity]['neutral_labels'] += 1\n",
        "\n",
        "    # Update overall sentiment based on counts\n",
        "    for entity in entities_dict:\n",
        "      positive_count = entities_dict[entity]['positive_labels']\n",
        "      negative_count = entities_dict[entity]['negative_labels']\n",
        "      neutral_count = entities_dict[entity]['neutral_labels']\n",
        "\n",
        "      if positive_count > negative_count and positive_count > neutral_count:\n",
        "        entities_dict[entity]['overall_sentiment'] = 'Positive'\n",
        "      elif negative_count > positive_count and negative_count > neutral_count:\n",
        "        entities_dict[entity]['overall_sentiment'] = 'Negative'\n",
        "      elif neutral_count > positive_count and neutral_count > negative_count:\n",
        "        entities_dict[entity]['overall_sentiment'] = 'Neutral'\n",
        "      else:\n",
        "        entities_dict[entity]['overall_sentiment'] = 'Mixed'\n",
        "\n",
        "#print(entities_dict)\n"
      ],
      "metadata": {
        "id": "2O8mP5aflXIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `9) Visualisation`:"
      ],
      "metadata": {
        "id": "QFcs6tLtlbFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialiser le map\n",
        "map_center = [0, 0]\n",
        "map_zoom = 2.5\n",
        "map = folium.Map(location=map_center, zoom_start=map_zoom, tiles='CartoDB Positron')\n",
        "\n",
        "# parcourir les entités\n",
        "for entity, info in entities_dict.items():\n",
        "  latitude = info['latitude']\n",
        "  longitude = info['longitude']\n",
        "  occurrences = info['occurrences']\n",
        "  overall_sentiment = info['overall_sentiment']\n",
        "  positive_labels = info['positive_labels']\n",
        "  negative_labels = info['negative_labels']\n",
        "  neutral_labels = info['neutral_labels']\n",
        "\n",
        "  # Déterminer la couleur de l'entité selon le sentiment général\n",
        "  if overall_sentiment == 'Positive':\n",
        "    color = 'green'\n",
        "  elif overall_sentiment == 'Negative':\n",
        "    color = 'red'\n",
        "  elif overall_sentiment == 'Neutral':\n",
        "    color = 'gray'\n",
        "  elif overall_sentiment == 'Mixed':\n",
        "    color = 'blueviolet'\n",
        "  else:\n",
        "    color = 'blue'\n",
        "  #print(overall_sentiment,\"  \", color)\n",
        "\n",
        "  scaling_factor = 3  # Ajuster le size scaling des icones\n",
        "  size = int(occurrences) * scaling_factor\n",
        "\n",
        "  # Créer la pop up des icones\n",
        "  popup =  f\"<div style='width: 110px; height: 80px;'>\"\n",
        "  popup += f\"<b>Entity:</b> {entity}<br>\"\n",
        "  popup += f\"<b>Occurrences:</b> {occurrences}<br>\"\n",
        "  popup += f\"<b>Sentiment:</b> {overall_sentiment}<br>\"\n",
        "  popup += f\"<b>Positive:</b> {positive_labels}<br>\"\n",
        "  popup += f\"<b>Negative:</b> {negative_labels}<br>\"\n",
        "  popup += f\"<b>Neutral:</b> {neutral_labels}<br>\"\n",
        "  popup +=  \"</div>\"\n",
        "\n",
        "  folium.CircleMarker(\n",
        "      location=[latitude, longitude],\n",
        "      radius=size,\n",
        "      popup=popup,\n",
        "      color=color,\n",
        "      fill=True,\n",
        "      fill_color=color,\n",
        "      #tooltip=f\"Entity: {entity}\"\n",
        "      tooltip=entity\n",
        "  ).add_to(map)\n",
        "\n",
        "\n",
        "# Création de la légende de tous les résultats:\n",
        "total_entities = len(entities_dict)\n",
        "total_positive = sum(info['positive_labels'] for info in entities_dict.values())\n",
        "total_negative = sum(info['negative_labels'] for info in entities_dict.values())\n",
        "total_neutral  = sum(info['neutral_labels'] for info in entities_dict.values())\n",
        "total_sentiments = total_positive + total_negative + total_neutral\n",
        "html_text = \"\"\"\n",
        "    <div style=\"position: absolute;\n",
        "                top: 50%; left: 0; transform: translate(0, -50%);\n",
        "                z-index: 1000; background-color: white; border: 2px solid #ccc;\n",
        "                padding: 10px; font-family: 'Trebuchet MS', sans-serif;\">\n",
        "    <h4>Annotation Results:</h4>\n",
        "    <b>Total Entities: {}<br>\n",
        "    <b>Total Sentiments: {}<br>\n",
        "    <ul>\n",
        "    <li>Positive: {}</li>\n",
        "    <li>Negative: {}</li>\n",
        "    <li>Neutral: {}</li>\n",
        "    </ul>\n",
        "    </div>\n",
        "    \"\"\".format(total_entities, total_sentiments, total_positive, total_negative, total_neutral)\n",
        "\n",
        "# Ajouter la légende à la carte\n",
        "map.get_root().html.add_child(folium.Element(html_text))\n",
        "\n",
        "map.save(\"map.html\") # enregistrer le fichier sur le disque\n",
        "map"
      ],
      "metadata": {
        "id": "ikI8x9GxlcMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#`Quelques liens utiles`:\n",
        "\n",
        "[Ariane](https://obtic.huma-num.fr/ariane/): plateforme d'analyse sémantique.\n",
        "\n",
        "[Obvie](https://obtic.huma-num.fr/obvie/): outil de fouille lexicale.\n",
        "\n",
        "[Elicom](https://obtic.huma-num.fr/elicom/): outil de fouille des correspondances.\n",
        "\n",
        "[Tanagra](https://obtic.sorbonne-universite.fr/tanagra/map): outil de cartographie.\n",
        "\n",
        "[Pandore](https://pandore-toolbox.isir.upmc.fr/): une boîte à outil pour les humanités numériques.\n",
        "..."
      ],
      "metadata": {
        "id": "_TgmI254KNj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#`Fin`\n",
        "\n",
        "<p xmlns:cc=\"http://creativecommons.org/ns#\" >Ce support est distribué selon les termes de la licence Creative Commons <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\">CC BY-NC-SA 4.0\n",
        "\n",
        "<img style=\"height:11px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\"><img style=\"height:11px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\"><img style=\"height:11px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1\"><img style=\"height:11px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\"></a></p>\n"
      ],
      "metadata": {
        "id": "EQYnENlVxwsd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}